# CUDA Graph

前几天和标哥在研究 torch-mem-savor 的时候发现自己对 CUDA Graph 一无所知，我只能模糊地理解到 CUDA Graph 是一种基于有向无环图的优化方式。恰好最近重新思考了下在 collcate 策略里的显存优化原理以及 SGLang 何时需要 flush cache，所以这篇文章快速理解下 CUDA Graph 的概念。

一些值得分享的文档：

- [Optimizing Memory Usage in verl](https://hebiao064.github.io/rl-memory-management)
- [How torch-memory-savor keep CUDA Graph](https://github.com/zhaochenyang20/torch_memory_saver-examples/blob/master/examples/rl_example.py)
- [When SGLang needs to flush cache](https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/verl/multi-turn/code-walk-through/readme.md#sglangrolloutasyncengine)

所以今天的文档尝试理解这些内容：

1. 什么是 CUDA Graph；
2. CUDA Graph 在推理中的应用；
3. 为什么 CUDA Graph 在训练中很少使用；
4. 为什么 torch-mem-savor 为了保护 CUDA Graph 需要额外保护 GPU 虚拟地址；
5. CUDA Graph 的显存大小一般是如何决定的；
6. CUDA Graph 和 torch compile 的异同；

## 什么是 CUDA Graph

CUDA Graph 是一种 NVIDIA CUDA 编程模型中的优化技术。它将一系列独立的 GPU 操作（比如核函数启动、内存拷贝、内存设置等）定义为一个有向无环图（DAG）。这个图中的每个节点代表一个 GPU 操作，而节点之间的边则表示操作的依赖关系。

一般而言，每一个 GPU 操作都需要由 CPU 发起一个单独的启动请求（kernel launch）。这种频繁的 CPU-GPU 交互会产生一定的 CPU 开销和延迟。当我们的计算流涉及大量微小、快速的 GPU 操作时，这些启动开销就会累积起来，成为显著的性能瓶颈。

总之，CUDA Graph 将多个 GPU 操作打包成一个可执行的图。一旦这个图被定义并实例化，CPU 只需要发出一次启动指令，就可以执行整个图中的所有操作，大幅减少了 CPU 和 GPU 之间的交互开销。

此外，一旦图被创建和实例化，你可以重复多次启动它，而不需要重新定义和实例化。这对于操作序列和依赖关系不变的静态工作流尤其有利。而较新的 CUDA 版本引入了条件节点等功能，允许图的一部分有条件地或重复地执行，而无需将控制权返回给 CPU。

## 为什么推理当中大量用到 CUDA Graph

让我惊讶的是，我长期见到 SGLang 保留着 1～2G 显存给 CUDA Graph，但是我并不知道为什么训练当中从未见过 CUDA Graph。因此我们分两部分内容来讨论，先讨论为什么推理能用，再讨论为什么训练很少用。

首先，对于推理而言，推理的流程是高度确定性的。

1. 模型结构固定： 一旦模型加载，其神经网络层（如 Transformer 层、全连接层、注意力机制等）的顺序和连接方式是固定的。
2. 计算图稳定： 对于给定的输入批次大小和最大序列长度，模型执行的计算图（一系列运算操作）通常是预先确定的，不会发生运行时结构性变化。
3. 重复执行： 大多数推理请求都会重复执行相同的模型前向计算，无论是一个短请求还是一个长序列的每一步生成。

这些操作通常是串行执行的，且单个操作的执行时间可能非常短。在传统的流式执行模型中，CPU 需要为每个小操作单独发起启动请求，导致频繁的 CPU-GPU 交互，产生大量的启动开销。


## 为什么 CUDA Graph 在训练中很少使用

反过来，更重要的问题是，为什么 FSDP 这样的分布式训练框架很少直接使用 CUDA Graph？我个人觉得主要是训练的不确定性显著高于推理。

1. 优化器（AdamW、SGD）对模型参数的更新通常是动态的，并可能涉及梯度裁剪、学习率调度等逻辑，这些逻辑可能根据训练步数、梯度值等动态变化。
2. 如果使用梯度累积，每次迭代的计算图可能会有细微的变化，因为累积到一定步数后才会进行优化器更新和梯度清零。而且，美中不足，基本大家都会用梯度累计。
3. 训练过程中通常包含 dropout、随机失活等随机操作，这些操作的图行为更难被静态捕获。
4. 训练过程中需要管理正向传播的激活值以及反向传播的梯度，这涉及到大量的内存分配和释放。CUDA Graph 需要在捕获时确定所有的内存需求。如果内存需求是动态变化的（例如，由于输入序列长度变化），则难以有效地使用 CUDA Graph。

我觉得第 4 点简直让我感到惊喜😂

## 为什么 torch-mem-savor 为了保护 CUDA Graph 需要额外保护 GPU 虚拟地址

理解这一点是关键！CUDA Graph 本身**不直接管理 GPU 的虚拟地址空间**。CUDA Graph 关注的是**操作的序列和依赖关系**，而不是内存的实际布局。它捕获的是一系列 GPU 操作（如核函数启动、内存拷贝、内存设置等）以及它们之间的依赖，以便这些操作能作为一个整体被提交给 GPU 执行。

然而，CUDA Graph 的执行**高度依赖于这些操作所引用的 GPU 内存的稳定性和可访问性**。这意味着：

1.  **内存指针的捕获：** 当你捕获一个 CUDA Graph 时，所有在核函数中引用的**内存指针（即 GPU 虚拟地址）**都会被记录在图中。这意味着在图捕获时，这些内存区域必须已经被分配好。
2.  **执行时的引用：** 当图被执行时，它会使用捕获时记录下来的那些内存指针。如果这些指针指向的内存区域在图执行时被释放或重新分配了（即使是分配了同样大小的内存，也很可能得到不同的虚拟地址），那么图的执行就会失败或产生错误的结果。

这就是 **`torch-mem-savor`** 之类的工具要解决的问题。在 PyTorch 中，当张量（Tensor）的生命周期结束时，PyTorch 的默认行为是调用 `cudaFree` 来释放其底层 GPU 内存。一旦 `cudaFree` 被调用，操作系统或驱动程序就可能将这块虚拟地址空间重新分配给其他用途。

为了让 CUDA Graph 能够重复执行而不会因为内存地址的变化而失效，`torch-mem-savor` 的核心思想是：**阻止 PyTorch 实际地释放 GPU 内存（即不调用 `cudaFree`）**。

它通过以下方式“保护”GPU 虚拟地址：

* **劫持内存释放操作：** 当 PyTorch 尝试释放一个张量时，`torch-mem-savor` 会拦截这个 `cudaFree` 调用。
* **将内存块放入自定义内存池：** 它不会真正释放这块内存，而是将这块已经不再被 PyTorch 逻辑使用的 GPU 内存块放入一个自定义的内存池中。
* **重复利用虚拟地址：** 当需要新的内存分配时，`torch-mem-savor` 会优先从自己的内存池中重用这些已经被标记为“空闲”但物理地址未变的内存块。

通过这种机制，`torch-mem-savor` 确保了在 CUDA Graph 捕获时记录的那些 GPU 虚拟地址在后续的图执行中始终是有效的，即使原始的张量已经被“逻辑上”释放了。这对于像强化学习中那种固定计算图、但中间变量会反复创建和销毁的场景尤其重要，因为它避免了反复的 `cudaMalloc` 和 `cudaFree` 调用，同时保持了 CUDA Graph 的有效性。

---

## 5. CUDA Graph 的显存大小一般是如何决定的

CUDA Graph 本身并不直接占用显存大小，它只是一个执行蓝图。真正占用显存的是图中所涉及的**数据**和**中间激活值**。因此，CUDA Graph 执行所需的显存大小，是由以下因素决定的：

1.  **模型参数大小：** 这是显存占用的基石。模型的权重、偏置等参数在整个推理或训练过程中都需要常驻显存。
2.  **输入和输出数据大小：** 你每次输入给模型的数据（如图像、文本序列的 Batch）以及模型产生的输出（如预测结果），都会占用显存。
3.  **中间激活值大小：** 这是决定显存占用上限的关键因素。在模型的前向传播过程中，每一层都会产生输出，这些输出通常被称为“激活值”。在执行 CUDA Graph 时，所有这些中间激活值都需要有足够的空间来存储。
4.  **工作空间（Workspace）/临时内存：** 某些算子（如一些高效的矩阵乘法库函数）可能需要额外的临时工作空间来执行计算。
5.  **KV Cache（仅限 LLM 推理）：** 在 LLM 推理中，为了加速生成过程，需要存储之前计算的 Key 和 Value 状态（KV Cache）。这个缓存的大小会随着生成序列的长度和并发请求的数量而增长，是 LLM 推理中主要的显存消耗之一。

**CUDA Graph 如何影响显存管理：**

CUDA Graph 在捕获阶段会记录所有显存操作，包括 `cudaMalloc`、`cudaMemcpy`、`cudaMemset` 等。当图被实例化并执行时，它会使用预先规划好的内存分配。

* **静态内存分配：** 在捕获 CUDA Graph 期间，所有需要分配的显存都必须被分配。这意味着图所需要的最大显存量是在捕获时就确定下来的。如果你的模型在推理时输入的形状是动态变化的（例如，序列长度不同），那么为了捕获一个可以处理所有可能形状的图，你可能需要为最坏情况（最大输入形状）预留显存，这可能导致内存的浪费。
* **内存池的结合：** 正如在 `torch-mem-savor` 中讨论的那样，为了避免频繁的显存分配和释放开销，同时保持 CUDA Graph 的有效性，通常会结合使用 **CUDA 内存池 (CUDA Memory Pool)**。内存池会预先向驱动程序申请一大块显存，然后在其中进行细粒度的分配和回收，而不会将显存真正归还给操作系统。这意味着在图执行期间，只要总的内存需求不超过内存池的大小，就可以高效地复用显存，而不会出现虚拟地址变化导致图失效的问题。
* **显存决定因素：** 因此，**CUDA Graph 本身不“决定”显存大小，而是计算图的拓扑结构和数据量“决定”了图执行所需的显存大小**。CUDA Graph 只是确保这些显存操作能够以最优化的方式执行，并依赖于这些显存地址的稳定性。

---

希望这份详尽的文档能够帮助你深入理解 CUDA Graph！如果你有其他问题，或者想进一步探讨 SGLang 中 flush cache 的细节，随时可以提问。